I"z<p>Many users of nanopolish <a href="https://twitter.com/BioMickWatson/status/870014676456927232">have noticed</a> that it takes quite a lot of compute time to polish a genome, particularly for very deep sequencing runs that are now common with R9.4 flowcells. I promised on twitter that I would work on that and the first set of optimizations have been pushed to the <a href="http://github.com/jts/nanopolish">nanopolish git repository</a>. In this short post Iâ€™ll explain where one of the bottlenecks was and how we fixed it.</p>

<h3 id="polishing-algorithm">Polishing Algorithm</h3>

<p>Nanopolish calculates an improved consensus sequence for a draft genome assembly by evaluating candidate edits using a signal-level hidden Markov model. The first stage of the algorithm examines the read alignments to discover where the genome assembly may contain errors. There are two phases to candidate generation. Large edits, like multiple inserted or deleted bases, are harvested from the aligned basecalled reads. We found that single-base edits are often missed in this phase, so a second pass exhaustively tests all possible single base substitutions, insertions and deletions using the HMM. For a genome of length <code class="highlighter-rouge">L</code> with average depth <code class="highlighter-rouge">d</code> this will make about <code class="highlighter-rouge">9Ld</code> calls to the hidden Markov model, which dominates nanopolishâ€™s runtime. Often nanopolish only ends up changing 0.2-1.0% of the assembly so most of the candidate edits are rejected. The new version of nanopolish has a smarter candidate testing algorithm that stops evaluating a candidate change when the log-likelihood ratio computed by the HMM reaches a threshold, <code class="highlighter-rouge">t</code>. This reduces the effect of depth (<code class="highlighter-rouge">d</code>) on runtime as most candidate edits have very poor log-likelihood ratios and are quickly rejected.</p>

<p>The new scoring algorithm uses a threshold of <code class="highlighter-rouge">t = 100</code> by default. For the less patient users there is a more aggressive option (<code class="highlighter-rouge">--faster</code>, which sets <code class="highlighter-rouge">t = 25</code>) but I want to test this further before making it the default.</p>

<p>The results of this new method are fairly dramatic on my benchmarking data set (E. coli R9.4 data at 300x depth):</p>

<table class="table table-striped table-post">
  <thead>
    <tr>
      <th>Version</th>
      <th>Percent Identity</th>
      <th>Runtime (CPU hours)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>v0.6.3</td>
      <td>99.95%</td>
      <td>4323</td>
    </tr>
    <tr>
      <td>v0.7.0</td>
      <td>99.95%</td>
      <td>574</td>
    </tr>
    <tr>
      <td>v0.7.0 â€“faster</td>
      <td>99.95%</td>
      <td>424</td>
    </tr>
    <tr>
      <td>v0.7.1 â€“faster</td>
      <td>99.96%</td>
      <td>91</td>
    </tr>
  </tbody>
</table>

<p>With default parameters 0.7.0 is 7.5x faster than the previous version on this assembly. With the <code class="highlighter-rouge">--faster</code> flag, it is over 10x faster. We have other optimizations planned so I hope we can reduce runtime even further - more hopefully soon.</p>

<p><strong>Update July 05</strong>: Weâ€™ve made two more improvements in nanopolish 0.7.1 to drop runtime on E. coli down to 91 CPU hours. The first change reduces the length of the event sequence and reference region that are input into the HMM during candidate variant screening. The second change stops trying to improve the consensus when the variant set converges rather than iterating for a fixed number of rounds.</p>
:ET